# Telegram Bot Configuration
# For development and production separation:
# - Set TELEGRAM_BOT_TOKEN_DEV for local development
# - Set TELEGRAM_BOT_TOKEN_PROD for production deployment
# - Or use TELEGRAM_BOT_TOKEN for both (legacy)
TELEGRAM_BOT_TOKEN_DEV=your_dev_telegram_bot_token_here
TELEGRAM_BOT_TOKEN_PROD=your_prod_telegram_bot_token_here
# Legacy (will be used if dev/prod tokens not set)
TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here

# ModelScope API Configuration
# Switch between two text models by changing MODELSCOPE_TEXT_MODEL_PROFILE

# API Credentials (same for both models)
MODELSCOPE_API_KEY=your_modelscope_api_key_here
MODELSCOPE_API_BASE=https://api-inference.modelscope.cn/v1

# Vision Model (shared by both profiles)
MODELSCOPE_VISION_MODEL=Qwen/Qwen3-VL-235B-A22B-Instruct

# Text Model Profile (options: glm4, qwen-coder)
MODELSCOPE_TEXT_MODEL_PROFILE=glm4

# Profile 1: GLM-4.6 (Default - Good for general tasks)
MODELSCOPE_TEXT_MODEL_GLM4=ZhipuAI/GLM-4.6

# Profile 2: Qwen3-Coder (Better for structured output, reasoning)
MODELSCOPE_TEXT_MODEL_QWEN_CODER=Qwen/Qwen3-Coder-480B-A35B-Instruct

# Supabase Configuration
# For development and production separation:
# - Set SUPABASE_URL_DEV and SUPABASE_KEY_DEV for local development
# - Set SUPABASE_URL_PROD and SUPABASE_KEY_PROD for production deployment
# - Or use SUPABASE_URL and SUPABASE_KEY for both (legacy)

# Development (Local Supabase)
SUPABASE_URL_DEV=http://127.0.0.1:54321
SUPABASE_KEY_DEV=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0
DATABASE_URL=postgresql://postgres:postgres@127.0.0.1:54322/postgres

# Production (Remote Supabase)
SUPABASE_URL_PROD=your_production_supabase_url_here
SUPABASE_KEY_PROD=your_production_supabase_key_here

# Legacy (will be used if dev/prod keys not set)
SUPABASE_URL=your_supabase_project_url_here
SUPABASE_KEY=your_supabase_anon_key_here

# Application Settings
CONFIDENCE_THRESHOLD=0.8
MAX_RETRIES=3
RETRY_DELAY=2000
LOG_LEVEL=info
NODE_ENV=development

# v2: Agent Loop Configuration (Multi-turn Conversations)
# Conversation management
CONVERSATION_EXPIRATION_HOURS=24
MAX_CONVERSATION_HISTORY=20

# Agent decision-making
CATEGORY_CONFIDENCE_THRESHOLD=0.8
EXTRACTION_CONFIDENCE_THRESHOLD=0.3

# Retry and timeout
AGENT_MAX_RETRIES=3
AGENT_RETRY_DELAY_MS=2000
AGENT_DECISION_TIMEOUT_MS=30000

# LLM configuration for agents
AGENT_LLM_MODEL=gpt-4o-mini
AGENT_LLM_TEMPERATURE=0.7
AGENT_LLM_MAX_TOKENS=500

# Database cleanup
CHECKPOINT_CLEANUP_INTERVAL_HOURS=6

# Feature flags
ENABLE_CONTEXT_INJECTION=true
ENABLE_ADAPTIVE_DECISIONS=true

# Memory Monitoring (Optional)
# Interval in milliseconds to log memory usage (default: 30000 = 30 seconds)
# Set to 0 to disable memory monitoring
MEMORY_MONITOR_INTERVAL_MS=30000

# LangSmith Tracing (Optional - for debugging LangGraph workflows)
# View traces at https://smith.langchain.com/
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=your_langsmith_api_key_here
# LANGCHAIN_PROJECT=receipt-tracker-agent
# LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
